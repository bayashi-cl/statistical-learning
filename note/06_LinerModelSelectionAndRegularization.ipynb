{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_LinerModelSelectionAndRegularization",
      "provenance": [],
      "authorship_tag": "ABX9TyPwSLxjomYy+XsK2hELuOjo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bayashi-cl/statistical-learning/blob/main/note/06_LinerModelSelectionAndRegularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 線形モデル選択と正則化\n",
        "\n",
        "この章では最小二乗法以外の線形モデルについて考える。\n",
        "\n",
        "* **部分集合選択**\\\n",
        "モデルに含まれる予測変数の集合のうち、最も「良い」部分集合を特定し、その部分集合に対して最小二乗法を適用する。\n",
        "* **縮小推定**\\\n",
        "予測変数をすべて使って推定をするが、何らかの操作により係数の推定値を0やそれに近い値にする。\n",
        "* **次元削減**\\\n",
        "$p$個の予測変数を$M(<p)$次元部分空間に射影して最小二乗法を適用する。\n",
        "\n",
        "これらのモデルは、最小二乗法に対して次のような利点がある。\n",
        "\n",
        "* **予測精度**\\\n",
        "最小二乗法はデータ数が予測変数の数よりも十分に大きければ十分な性能を発揮するが、そうでない場合は過学習などの問題が発生して分散が大きくなる。縮小推定では回帰係数に制約を課すことで予測精度を向上させる。\n",
        "* **モデルの解釈可能性**\\\n",
        "モデルの中に実際には応答変数と関係のない予測変数が含まれていると、モデルが必要以上に複雑になってしまう。このような無意味な変数を取り除くことでモデルの解釈が容易になる。"
      ],
      "metadata": {
        "id": "qlckyD4x6CuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 部分集合選択"
      ],
      "metadata": {
        "id": "stqDCdtXAFG-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 補足 計算量の評価について\n",
        "\n",
        "アルゴリズムのコスト（計算回数）のことを（時間）計算量と呼ぶ。計算量を評価する際に、例えば入力サイズが$n$のアルゴリズムの計算コストが$3n^2 + 2n + 5$である場合、注目するべきは$n^2$の部分であり、それ以外の係数や1次より小さい項は$n$が増加した場合の計算量の増加度合いにはあまり関与しない。\n",
        "\n",
        "数列の**オーダー**の記法を計算量でも用いる。\n",
        "\n",
        "#### $\\Theta(O,o,\\Omega , \\omega)$記法\n",
        "\n",
        "2つの計算量の増加度合いが同じ（くらい）であることを言いたい。\n",
        "\n",
        "$f(n)$が$g(n)$に対して次の条件を満たすとき、$f(n)$のオーダーが$g(n)$であるといい、$f(n) \\in \\Theta (g(n))$ と表記する。\n",
        "\n",
        "> ある定数 $n_0$​ に対し、ある定数 $c_L, c_U > 0$ が存在し、$n_0$ 以上のすべての $n$ について次の式が成り立つ：\n",
        "> $$0 \\le c_L \\cdot g(n) \\le f(n) \\le c_U \\cdot g(n)$$\n",
        "\n",
        "また、上の不等式の$c_L$側が成り立つとき、$f(n) \\in \\Omega (g(n))$、$c_U$側が成り立つとき、$f(n) \\in O (g(n))$と表記する。\n",
        "\n",
        "更に、大小関係が心に成り立つ（$\\le$ではなく$\\lt$）場合は,\n",
        "それぞれ$f(n) \\in \\omega (g(n))$、$f(n) \\in o (g(n))$と表記する。\n",
        "\n",
        "#### 参考文献\n",
        "\n",
        "* アルゴリズムイントロダクション 第3版 総合版(p36-)\\\n",
        "該当部分は[Amazon](https://www.amazon.co.jp/dp/B078WPYHGN)で試し読みができます。\n",
        "\n"
      ],
      "metadata": {
        "id": "jVrR79WdAcA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1.1 最良部分集合選択\n",
        "\n",
        "すべての予測変数の組み合わせを評価する手法。以下のアルゴリズムで行われる。\n",
        "\n",
        "1. $M_0$を予測変数を持たない**ヌルモデル**とする。ヌルモデルは予測値として標本平均を返す。\n",
        "1. $k = 1, 2, \\ldots , p$について以下の手順を行う。\n",
        "    1. $k$個の予測変数を持つ$\\binom{p}{k}$個のモデルに対してRSSや$R^2$を計算する。\n",
        "    1. 最良のモデルを$M_k$とする。\n",
        "1. $M_0, \\ldots , M_k$から最良のモデルをBICや修正$R^2$などで選ぶ。\n",
        "\n",
        "step3でのモデル選択の際にはRSSや$R^2$が特徴の数に対して単調減少/増加することに注意する。RSSが小さい・$R^2$が大きいことは単に訓練データに対して誤差が小さいことを示しているということであり、重要なのはテストデータに対する誤差である。\n",
        "\n",
        "最良部分集合選択は単純であり、得られるモデルも厳密に最良であるといえるが、計算量のオーダーが$\\Omega(2^n)$であり、$p \\ge 40$の場合には現実的な時間で解くことが難しくなる。"
      ],
      "metadata": {
        "id": "vfDoKe6FSzP2"
      }
    }
  ]
}